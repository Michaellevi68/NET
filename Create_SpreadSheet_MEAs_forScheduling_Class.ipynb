{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to convert, backup and log experiments\n",
    "\n",
    "* Requires MCDS commandline conversion tool\n",
    "* Requires GCE commandline tools and setup for account with access to storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pickle\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import concurrent.futures as cf\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "class convertuploaddocument:\n",
    "    \"\"\"Class which allows for upload and batch conversion\n",
    "    of mcds files generated by MultiChannel Systems Experimenter\n",
    "    The class is meant to work on the MCS recording computer in Windows 10.\n",
    "    Version 1: 10-06-19 Noah Dolev\n",
    "    Version 1.1: 11-06-19 Noah Dolev [added spreadsheet export]\n",
    "    \"\"\"\n",
    "\n",
    "    def walk(self):\n",
    "        \"\"\"Get all files in path\n",
    "        \"\"\"\n",
    "        for p, d, f in os.walk(self.searchpath):\n",
    "            for ff in f:\n",
    "                if ff.endswith(self.suffix):\n",
    "                    self.files.append(os.path.join(p, ff))\n",
    "\n",
    "    def scanforexperiments(self):\n",
    "        \"\"\"Takes a search path and looks recursively for all MSRD files\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.info('Start: Scan for new files')\n",
    "        self.walk()\n",
    "        self.logger.info('Process: %d _total_ files found' % len(self.files))\n",
    "        oldfilelist = []\n",
    "\n",
    "        # Checks if this was run in the past, if so, only processes the new files\n",
    "        oldfileslog = os.path.join(self.logpath, 'filelist.p')\n",
    "        if self.startfresh:\n",
    "            self.logger.info('Start: Erasing File History')\n",
    "            pickle_out = open(oldfileslog, \"wb\")\n",
    "            pickle.dump([], pickle_out)\n",
    "            pickle_out.close()\n",
    "\n",
    "        if os.path.getsize(oldfileslog) > 0:\n",
    "            pickle_in = open(oldfileslog, \"rb\")\n",
    "            oldfilelist = pickle.load(pickle_in)\n",
    "            pickle_in.close()\n",
    "            self.logger.info('Process: %d _old_ files found' % len(oldfilelist))\n",
    "\n",
    "        pickle_out = open(oldfileslog, \"wb\")\n",
    "        pickle.dump(self.files, pickle_out)\n",
    "        pickle_out.close()\n",
    "        self.logger.info('Process: Old files list updated')\n",
    "\n",
    "        if len(oldfilelist) != 0:\n",
    "            self.files = list(set(self.files) - set(oldfilelist))\n",
    "        self.numfiles = len(self.files)\n",
    "        self.logger.info('Process: %d _new_ files found' % self.numfiles)\n",
    "        self.logger.info('End: Scan for files')\n",
    "\n",
    "    def parsefields(self, getvalchar='<', nextfieldchar='_'):\n",
    "        self.field['fullpathmsrd'] = self.files * 2\n",
    "        self.field.sort_values(by=['fullpathmsrd'], inplace=True)\n",
    "        self.field.reset_index(drop=True, inplace=True)\n",
    "        self.field['MEAfiles'] = self.field['fullpathmsrd'].apply(\n",
    "            lambda x: x.split('\\\\')[-1].replace(self.suffix, 'h5'))\n",
    "        self.field['MEAfiles'].loc[::2] = self.field['MEAfiles'].loc[::2].apply(lambda x: x.replace('h5', 'bin'))\n",
    "        self.field['folder'] = ['\\\\' + os.path.join(*word[:-1]) for word in\n",
    "                                [f.split('\\\\') for f in self.field['fullpathmsrd']]]\n",
    "        self.field['recNames'] = self.field['MEAfiles'].apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "\n",
    "        def setrecformat(x):\n",
    "            if 'h5' in x:\n",
    "                return ('MCH5Recording')\n",
    "            else:\n",
    "                return ('binaryRecording')\n",
    "\n",
    "        self.field['recFormat'] = [setrecformat(x) for x in self.field['MEAfiles']]\n",
    "\n",
    "        flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "        nvc = '|'.join(flatten([nextfieldchar]))\n",
    "        gvc = '|'.join(flatten([getvalchar]))\n",
    "        for ind in self.field.index:\n",
    "            word = self.field['fullpathmsrd'].loc[ind].split('\\\\')\n",
    "            keyvalpair = [re.split(nvc, w) for w in word]\n",
    "            for kv in keyvalpair:\n",
    "                for k in kv:\n",
    "                    if '=' in k:\n",
    "                        temp = re.split(gvc, k)[0]\n",
    "                        temp = temp[-np.min([6, len(temp)]):]  # maximum field length is 6 characters\n",
    "                        key = ''.join(j for j in temp if not j.isdigit())  #\n",
    "                        temp = re.split(gvc, k)[1:]\n",
    "                        val = ''.join(temp)\n",
    "                        if key not in self.field.columns:\n",
    "                            self.field[key] = 'unspecified'\n",
    "                        self.field[key].loc[ind] = val.split('.' + self.suffix)[0]\n",
    "        self.field.fillna('unspecified', inplace=True)\n",
    "\n",
    "    def makeGSdir(self, directory='database/'):\n",
    "        \"\"\"Creates a google storage bucket for data\n",
    "        \"\"\"\n",
    "        blob = self.bucket.blob(directory.replace('\\\\', '/'))\n",
    "        blob.upload_from_string('', content_type='application/x-www-form-urlencoded;charset=UTF-8')\n",
    "\n",
    "    def createDirStructure(self):\n",
    "        \"\"\"Parses pandas dataframe into a directory structure for easy reading by a database engine\n",
    "        whereby each field and value are changed into \"x=y\" directories.\n",
    "        _Should add code to automatically form the directory hierarchy so that the smallest number of files is in the lowest folder_\n",
    "        \"\"\"\n",
    "        table = self.field\n",
    "        table = table[table['MEAfiles'].str.contains('h5')].drop(\n",
    "            ['fullpathmsrd', 'exclude', 'recFormat', 'recNames', 'folder', 'comments'], axis=1)\n",
    "        table = table.reindex(columns=(['user'] + list([col for col in table.columns if col != 'user'])))\n",
    "        dirs = [t for t in table.columns.values if t != 'MEAfiles']\n",
    "        for i in range(0, table.shape[0]):\n",
    "            dtemp = 'database'\n",
    "            for d in dirs:\n",
    "                if 'unspecified' not in table[d].iloc[i]:\n",
    "                    dtemp = os.path.join(dtemp, d + '=%s' % table[d].iloc[i])\n",
    "            self.directory.append(dtemp.replace('\\\\', '/'))\n",
    "\n",
    "    def uploadFile(self, file, path, bucket):\n",
    "        \"\"\"Uploads files to Google Cloud Storage\n",
    "        :param file: the file to be uploaded\n",
    "        :param path: the generated path using the file's inferred experiment attributes\n",
    "        :param bucket: the bucket to place the file\n",
    "        \"\"\"\n",
    "        path = path + '\\\\'\n",
    "        path = path.replace('\\\\', '/')\n",
    "        testexists = bucket.blob(path + file.replace('.msrd', '.h5').split('\\\\')[-1])\n",
    "        if not testexists.exists():\n",
    "            self.makeGSdir(directory=path)\n",
    "            bashCommand = 'gsutil -o GSUtil:parallel_composite_upload_threshold=150M -m cp \"%s\" \"gs://%s\"' % (\n",
    "                file.replace('.msrd', '.h5'), path)\n",
    "            # Multi-threaded composite upload. Still quite slow.\n",
    "            subprocess.Popen(bashCommand, stdout=subprocess.PIPE, shell=True)\n",
    "\n",
    "    def multi_part_upload_with_s3(self, file_path, key_path, bucket=\"meadata\"):\n",
    "        \"\"\"Uploads files in parallel to Amazon s3 buckets\n",
    "        :param file_path: path of file to upload\n",
    "        :param key_path: generated path using the file's inferred experiment attributes\n",
    "        :param bucket: bucket to place the file\n",
    "        \"\"\"\n",
    "        # Multipart upload\n",
    "        with open(self.awsaccesskey, 'rb') as csvfile:\n",
    "            keys = pd.read_csv(csvfile)\n",
    "        s3 = boto3.resource('s3', aws_access_key_id=keys[\"Access key ID\"][0],\n",
    "                            aws_secret_access_key=keys[\"Secret access key\"][0])\n",
    "        config = TransferConfig(multipart_threshold=1024 * 25, max_concurrency=8,\n",
    "                                multipart_chunksize=1024 * 25, use_threads=True)\n",
    "        b = s3.Bucket(bucket)\n",
    "        objs = list(b.objects.filter(Prefix=key_path))\n",
    "        if len(objs) > 0 and objs[0].key == key_path:\n",
    "            print(\"%s already Exists - skipping upload!\" % file_path.split('\\\\')[-1])\n",
    "        else:\n",
    "            s3.meta.client.upload_file(Filename=file_path, Bucket=bucket, Key=key_path,\n",
    "                                       Config=config)\n",
    "\n",
    "    def localnetworkcopy(self, f, d, archive=True):\n",
    "        \"\"\"\n",
    "        :param f: file to copy\n",
    "        :param d: directory to place file\n",
    "        :param archive: Flag for whether to also copy file to local network archive\n",
    "        \"\"\"\n",
    "        self.logger.info('Process: Copy h5 to local network location')\n",
    "        subprocess.call(\n",
    "            [\"xcopy\", f.replace(\"msrd\", \"h5\"), os.path.join(self.localsharepath, d.replace('/', '\\\\')) + '\\\\',\n",
    "             \"/K/O/X/i\"])\n",
    "        self.logger.info('Process: %s copied succesfully' % f.replace(\"msrd\", \"h5\"))\n",
    "        if archive:\n",
    "            self.logger.info('Process: Copy MSRD and MSRS to local network archive')\n",
    "            subprocess.call(\n",
    "                [\"xcopy\", f, os.path.join(self.localarchivepath, d.replace('/', '\\\\')) + '\\\\', \"/K/O/X/i/Y\"])\n",
    "            subprocess.call(\n",
    "                [\"xcopy\", f.replace(\"msrd\", \"msrs\"), os.path.join(self.localarchivepath, d.replace('/', '\\\\')) + '\\\\',\n",
    "                 \"/K/O/X/i/Y\"])\n",
    "            self.logger.info('Process: %s msrd and msrs copied succesfully to archive' % f.split('.')[0])\n",
    "\n",
    "    def createspreadsheet(self):\n",
    "        \"\"\"Takes parsed file dataframe and creates a spreadsheet based on the fields\n",
    "            :return pandas dataframe with list of uploaded files and their metadata\n",
    "        \"\"\"\n",
    "        self.field = self.field.replace(\"unspecified\", \"\")\n",
    "        spreadsheetpath = os.path.join(self.searchpath, 'experiments.xlsx')\n",
    "        self.field.to_excel(spreadsheetpath, index=False)\n",
    "        if self.cloudflag == \"gcs\":\n",
    "            self.logger.info('Process: Starting upload of spreadsheet to GS')\n",
    "            self.uploadFile(file=spreadsheetpath, path='/database',\n",
    "                            bucket=self.bucket)\n",
    "            self.logger.info('Process: Successfully uploaded spreadsheet to GS')\n",
    "        elif self.cloudflag == \"aws\":\n",
    "            with open(self.awsaccesskey, 'rb') as csvfile:\n",
    "                keys = pd.read_csv(csvfile)\n",
    "            self.multi_part_upload_with_s3(file_path=spreadsheetpath, key_path=\"database/experiments.xlsx\")\n",
    "            self.logger.info(\"Process: Spreadsheet uploaded to AWS s3\")\n",
    "        else:\n",
    "            self.logger.info('Process: Not uploading spreadsheet to cloud storage')\n",
    "        if self.localcopyflag == True:\n",
    "            self.localnetworkcopy(spreadsheetpath, self.localsharepath)\n",
    "        return (self.field)\n",
    "\n",
    "    def converttoh5(self):\n",
    "        \"\"\"Takes files that were discovered and converts them to h5 then uploads them to google cloud / amazon S3 and/or makes a local network copy\n",
    "        \"\"\"\n",
    "\n",
    "        def processfiles(f, d, bucket=self.bucket, mcspath=self.mcspath, cloudflag=self.cloudflag,\n",
    "                         localcopyflag=self.localcopyflag):\n",
    "            \"\"\"Internal function for multiprocessing conversion and upload\n",
    "            :param f: file to process\n",
    "            :param d: generated path by createDirStructure\n",
    "            :param bucket: bucket to place file\n",
    "            :param mcspath: path to MCS commandline conversion tool\n",
    "            :param cloudflag: flag whether to use google (\"gcs\", \"aws\" or \"local\")\n",
    "            :param localcopyflag: if True, also copies file to a local network direction\n",
    "            \"\"\"\n",
    "\n",
    "            if not os.path.isfile(f.replace('.msrd', '.h5')):\n",
    "                bashCommand = '%s -t hdf5 \"%s\"' % (mcspath, f.replace('.msrd', '.msrs'))\n",
    "                process = subprocess.Popen(bashCommand, stdout=subprocess.PIPE)\n",
    "                output, error = process.communicate()\n",
    "                if error is not None:\n",
    "                    self.logger.error('File failed to convert with error: \\n %s' % error)\n",
    "                else:\n",
    "                    self.logger.info('Process: Successfully converted file to H5')\n",
    "                    # Workaround since there is no way to specify output file name with MCS commandline tool\n",
    "                    os.rename(f.replace('.msrd', '.h5'), os.path.join('//'.join(f.split('//')[:-1]), hashlib.md5(\n",
    "                        f.split('//')[-1].split('.')[0].encode()).hexdigest() + '.h5'))\n",
    "                    self.logger.info('Process: File renamed to md5 hash successfully')\n",
    "                    f = os.path.join('//'.join(f.split('//')[:-1]),\n",
    "                                     hashlib.md5(f.split('//')[-1].split('.')[0].encode()).hexdigest() + '.h5')\n",
    "\n",
    "            if (cloudflag == \"gcs\"):\n",
    "                self.logger.info('Process: Starting upload of HDF5 to target directory of GS')\n",
    "                self.uploadFile(file=f, path=d,\n",
    "                                bucket=bucket)\n",
    "                # can be improved by composing a composite object containing all the files to upload\n",
    "                self.logger.info('Process: Successfully uploaded H5 to GS')\n",
    "            elif (cloudflag == \"aws\"):\n",
    "                with open(\"D:\\\\code\\\\user=ND\\\\ND_AccessKey.csv\", 'rb') as csvfile:\n",
    "                    keys = pd.read_csv(csvfile)\n",
    "                file_path = f.replace('.msrd', '.h5')\n",
    "                key_path = d + '/' + f.replace('.msrd', '.h5').split('\\\\')[-1]\n",
    "                self.logger.info('Process: %s Uploading' % file_path)\n",
    "                self.logger.info('Process: File uploading to: %s' % key_path)\n",
    "                self.multi_part_upload_with_s3(file_path=file_path, key_path=key_path, bucket=\"meadata\")\n",
    "            else:\n",
    "                self.logger.info('Process: Not uploading to cloud storage')\n",
    "            if (localcopyflag == True):\n",
    "                self.localnetworkcopy(f, d)\n",
    "\n",
    "        self.logger.info('Start: Conversion from MSDS to H5')\n",
    "        self.parsefields(getvalchar=['=', '>'], nextfieldchar=[',', '_'])\n",
    "        self.createDirStructure()\n",
    "        self.logger.info('Process: Created target directory structure')\n",
    "        with cf.ProcessPoolExecutor() as executor:\n",
    "            _ = [executor.submit(processfiles(f=self.files[i], d=self.directory[i])) for i in range(0, len(self.files))]\n",
    "        self.logger.info('End: Conversion from MSDS to H5')\n",
    "\n",
    "    def __init__(self, searchpath=\"D:\\\\Multi Channel DataManager\\\\\", startfresh=False,\n",
    "                 suffix='.msrd', gcs_credentials_path=\"D:\\\\code\\\\user=ND\\\\divine-builder-142611-9884de65797a.json\",\n",
    "                 gcs_project_id='divine-builder-142611', bucketname='meadata',\n",
    "                 logpath=os.path.join('d:\\\\', 'code', 'user=ND'),\n",
    "                 mcspath=os.path.join('d:\\\\', 'code', 'user=ND', 'McsDataCommandLineConverter',\n",
    "                                      \"McsDataCommandLineConverter.exe\"),\n",
    "                 cloudflag=\"aws\", localcopyflag=True, localsharepath=\"\\\\\\\\132.77.73.171\\\\MEA_DATA\\\\\",\n",
    "                 localarchivepath=\"\\\\\\\\data.wexac.weizmann.ac.il\\\\rivlinlab-arc\\\\\",\n",
    "                 awsaccesskey=\"D:\\\\code\\\\user=ND\\\\ND_AccessKey.csv\"):\n",
    "        \"\"\"Initialize class\n",
    "        :param searchpath: path to directory with data files\n",
    "        :param startfresh: flag, if 1 then attempt to upload all files\n",
    "        :param suffix: suffix of data files\n",
    "        :param gcs_credentials_path: path to google cloud credential file\n",
    "        :param gcs_project_id: name of google cloud project\n",
    "        :param bucketname: name of bucket to upload files\n",
    "        :param logpath: path to save log file\n",
    "        :param mcspath: path to mcs data command line converter\n",
    "        :param cloudflag: flag, if 1 then attempt to upload files to google\n",
    "        :param localsharepath: path to local network shared directory\n",
    "        \"\"\"\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = gcs_credentials_path\n",
    "        self.gcs_client = storage.Client(project=gcs_project_id)\n",
    "        self.bucket = self.gcs_client.get_bucket(bucketname)\n",
    "\n",
    "        self.logpath = logpath\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        # create a file handler\n",
    "        self.handler = logging.FileHandler(os.path.join(self.logpath, 'SpreadSheet_RunLog.log'), mode='a')\n",
    "        self.handler.setLevel(logging.INFO)\n",
    "\n",
    "        # create a logging format\n",
    "        self.formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        self.handler.setFormatter(self.formatter)\n",
    "\n",
    "        # add the handlers to the logger\n",
    "        self.logger.addHandler(self.handler)\n",
    "        self.files = []\n",
    "        colnames = ['fullpathmsrd', 'exclude', 'folder', 'coating', 'cleaning', 'MEAfiles', 'recFormat', 'recNames',\n",
    "                    'comments']\n",
    "        self.field = pd.DataFrame(columns=colnames)\n",
    "        self.searchpath = searchpath\n",
    "        self.startfresh = startfresh\n",
    "        self.suffix = suffix\n",
    "        self.numfiles = 0\n",
    "        self.directory = []\n",
    "        self.mcspath = mcspath\n",
    "        self.cloudflag = cloudflag\n",
    "        self.localcopyflag = localcopyflag\n",
    "        self.localsharepath = localsharepath\n",
    "        self.localarchivepath = localarchivepath\n",
    "        self.awsaccesskey = awsaccesskey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cud = convertuploaddocument(startfresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cud.scanforexperiments()\n",
    "cud.parsefields(getvalchar=['=', '>'], nextfieldchar=[',', '_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cud.createDirStructure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a = cud.createspreadsheet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cud.localnetworkcopy(cud.files[0], cud.directory[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cud.directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
