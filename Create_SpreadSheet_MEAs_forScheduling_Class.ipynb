{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to convert, backup and log experiments\n",
    "\n",
    "* Requires MCDS commandline conversion tool\n",
    "* Requires GCE commandline tools and setup for account with access to storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import logging\n",
    "import subprocess\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import concurrent.futures as cf\n",
    "import threading\n",
    "import boto3\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "import hashlib\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "\n",
    "\n",
    "class convertuploaddocument:\n",
    "\n",
    "    def walk(self):\n",
    "        for p, d, f in os.walk(self.searchpath):\n",
    "            for ff in f:\n",
    "                if ff.endswith(self.suffix):\n",
    "                    self.files.append(os.path.join(p, ff))\n",
    "\n",
    "    def scanforexperiments(self):\n",
    "        '''\n",
    "        Takes a search path and looks recursively for all MSRD files\n",
    "        '''\n",
    "\n",
    "        self.logger.info('Start: Scan for new files')\n",
    "        self.walk()\n",
    "        self.logger.info('Process: %d _total_ files found' % len(self.files))\n",
    "        oldfilelist = []\n",
    "\n",
    "        # Checks if this was run in the past, if so, only processes the new files\n",
    "        oldfileslog = os.path.join(self.logpath, 'filelist.p')\n",
    "        if self.startfresh:\n",
    "            self.logger.info('Start: Erasing File History')\n",
    "            pickle_out = open(oldfileslog, \"wb\")\n",
    "            pickle.dump([], pickle_out)\n",
    "            pickle_out.close()\n",
    "\n",
    "        if os.path.getsize(oldfileslog) > 0:\n",
    "            pickle_in = open(oldfileslog, \"rb\")\n",
    "            oldfilelist = pickle.load(pickle_in)\n",
    "            pickle_in.close()\n",
    "            self.logger.info('Process: %d _old_ files found' % len(oldfilelist))\n",
    "\n",
    "        pickle_out = open(oldfileslog, \"wb\")\n",
    "        pickle.dump(self.files, pickle_out)\n",
    "        pickle_out.close()\n",
    "        self.logger.info('Process: Old files list updated')\n",
    "\n",
    "        if len(oldfilelist) != 0:\n",
    "            self.files = list(set(self.files) - set(oldfilelist))\n",
    "        self.numfiles = len(self.files)\n",
    "        self.logger.info('Process: %d _new_ files found' % self.numfiles)\n",
    "        self.logger.info('End: Scan for files')\n",
    "\n",
    "    def parsefields(self, getvalchar='<', nextfieldchar='_'):\n",
    "        self.field['fullpathmsrd'] = self.files * 2\n",
    "        self.field.sort_values(by=['fullpathmsrd'], inplace=True)\n",
    "        self.field.reset_index(drop=True, inplace=True)\n",
    "        self.field['MEAfiles'] = self.field['fullpathmsrd'].apply(\n",
    "            lambda x: x.split('\\\\')[-1].replace(self.suffix, 'h5'))\n",
    "        self.field['MEAfiles'].loc[::2] = self.field['MEAfiles'].loc[::2].apply(lambda x: x.replace('h5', 'bin'))\n",
    "        self.field['folder'] = ['\\\\' + os.path.join(*word[:-1]) for word in\n",
    "                                [f.split('\\\\') for f in self.field['fullpathmsrd']]]\n",
    "        self.field['recNames'] = self.field['MEAfiles'].apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "\n",
    "        def setrecformat(x):\n",
    "            if 'h5' in x:\n",
    "                return ('MCH5Recording')\n",
    "            else:\n",
    "                return ('binaryRecording')\n",
    "\n",
    "        self.field['recFormat'] = [setrecformat(x) for x in self.field['MEAfiles']]\n",
    "\n",
    "        flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "        nvc = '|'.join(flatten([nextfieldchar]))\n",
    "        gvc = '|'.join(flatten([getvalchar]))\n",
    "        # words = [f.split('\\\\') for f in field['fullpathmsrd']]\n",
    "        # keyvalpair = []\n",
    "        for ind in self.field.index:\n",
    "            word = self.field['fullpathmsrd'].loc[ind].split('\\\\')\n",
    "            keyvalpair = [re.split(nvc, w) for w in word]\n",
    "            for kv in keyvalpair:\n",
    "                for k in kv:\n",
    "                    if '=' in k:\n",
    "                        temp = re.split(gvc, k)[0]\n",
    "                        temp = temp[-np.min([6, len(temp)]):]  # maximum field length is 6 characters\n",
    "                        key = ''.join(j for j in temp if not j.isdigit())  #\n",
    "                        temp = re.split(gvc, k)[1:]\n",
    "                        val = ''.join(temp)\n",
    "                        if key not in self.field.columns:\n",
    "                            self.field[key] = 'unspecified'\n",
    "                        self.field[key].loc[ind] = val.split('.' + self.suffix)[0]\n",
    "        self.field.fillna('unspecified', inplace=True)\n",
    "\n",
    "    def makeGSdir(self, directory='database/'):\n",
    "        blob = self.bucket.blob(directory.replace('\\\\', '/'))\n",
    "        blob.upload_from_string('', content_type='application/x-www-form-urlencoded;charset=UTF-8')\n",
    "\n",
    "    def createDirStructure(self):\n",
    "        table = self.field\n",
    "        table = table[table['MEAfiles'].str.contains('h5')].drop(\n",
    "            ['fullpathmsrd', 'exclude', 'recFormat', 'recNames', 'folder', 'comments'], axis=1)\n",
    "        table = table.reindex(columns=(['user'] + list([col for col in table.columns if col != 'user'])))\n",
    "        dirs = [t for t in table.columns.values if t != 'MEAfiles']\n",
    "        for i in range(0, table.shape[0]):\n",
    "            dtemp = 'database'\n",
    "            for d in dirs:\n",
    "                if 'unspecified' not in table[d].iloc[i]:\n",
    "                    dtemp = os.path.join(dtemp, d + '=%s' % table[d].iloc[i])\n",
    "            self.directory.append(dtemp.replace('\\\\', '/'))\n",
    "\n",
    "    def uploadFile(self, file, path, bucket):\n",
    "        path = path + '\\\\'\n",
    "        path = path.replace('\\\\', '/')\n",
    "        testexists = bucket.blob(path + file.replace('.msrd', '.h5').split('\\\\')[-1])\n",
    "        if not testexists.exists():\n",
    "            self.makeGSdir(directory=path)\n",
    "            bashCommand = 'gsutil -o GSUtil:parallel_composite_upload_threshold=150M -m cp \"%s\" \"gs://%s\"' % (\n",
    "            file.replace('.msrd', '.h5'), path)  # +file.split('\\\\')[-1].replace('.msrd','.h5')\n",
    "            # Multi-threaded composite upload. Still quite slow.\n",
    "            subprocess.Popen(bashCommand, stdout=subprocess.PIPE, shell=True)\n",
    "\n",
    "    def multi_part_upload_with_s3(self, file_path, key_path, bucket=\"meadata\"):\n",
    "        # Multipart upload\n",
    "        with open(\"D:\\\\code\\\\user=ND\\\\ND_AccessKey.csv\", 'rb') as csvfile:\n",
    "            keys = pd.read_csv(csvfile)\n",
    "        s3 = boto3.resource('s3', aws_access_key_id=keys[\"Access key ID\"][0],\n",
    "                            aws_secret_access_key=keys[\"Secret access key\"][0])\n",
    "        config = TransferConfig(multipart_threshold=1024 * 25, max_concurrency=8,\n",
    "                                multipart_chunksize=1024 * 25, use_threads=True)\n",
    "        b = s3.Bucket(bucket)\n",
    "        objs = list(b.objects.filter(Prefix=key_path))\n",
    "        if len(objs) > 0 and objs[0].key == key_path:\n",
    "            print(\"%s already Exists - skipping upload!\" % file_path.split('\\\\')[-1])\n",
    "        else:\n",
    "            s3.meta.client.upload_file(Filename=file_path, Bucket=bucket, Key=key_path,\n",
    "                                       Config=config)\n",
    "\n",
    "    def converttoh5(self):\n",
    "        '''\n",
    "        Takes files that were discovered and converts them to h5 then uploads them to google cloud\n",
    "        '''\n",
    "\n",
    "        def processfiles(f, d, bucket=self.bucket, mcspath=self.mcspath, google=self.gcs):\n",
    "            if not os.path.isfile(f.replace('.msrd', '.h5')):\n",
    "                bashCommand = '%s -t hdf5 \"%s\"' % (mcspath, f.replace('.msrd', '.msrs'))\n",
    "                process = subprocess.Popen(bashCommand, stdout=subprocess.PIPE)\n",
    "                output, error = process.communicate()\n",
    "                if error is not None:\n",
    "                    self.logger.error('File failed to convert with error: \\n %s' % error)\n",
    "                else:\n",
    "                    self.logger.info('Process: Successfully converted file to H5')\n",
    "            if (google == 1):\n",
    "                self.logger.info('Process: Starting upload of HDF5 to target directory of GS')\n",
    "                self.uploadFile(file=f, path=d,\n",
    "                                bucket=bucket)  # can be improved by composing a composite object containing all the files to upload\n",
    "                self.logger.info('Process: Succesfully uploaded H5 to GS')\n",
    "            else:\n",
    "                with open(\"D:\\\\code\\\\user=ND\\\\ND_AccessKey.csv\", 'rb') as csvfile:\n",
    "                    keys = pd.read_csv(csvfile)\n",
    "                file_path = f.replace('.msrd', '.h5')\n",
    "                key_path = d + '/' + f.replace('.msrd', '.h5').split('\\\\')[-1]\n",
    "                self.logger.info('Process: %s Uploading' % file_path)\n",
    "                self.logger.info('Process: File uploading to: %s' % key_path)\n",
    "                self.multi_part_upload_with_s3(file_path=file_path, key_path=key_path, bucket=\"meadata\")\n",
    "\n",
    "        self.logger.info('Start: Conversion from MSDS to H5')\n",
    "        self.parsefields(getvalchar=['=', '>'], nextfieldchar=[',', '_'])\n",
    "        self.createDirStructure()\n",
    "        self.logger.info('Process: Created target directory structure')\n",
    "        with cf.ProcessPoolExecutor() as executor:\n",
    "            _ = [executor.submit(processfiles(f=self.files[i], d=self.directory[i])) for i in range(0, len(self.files))]\n",
    "        self.logger.info('End: Conversion from MSDS to H5')\n",
    "\n",
    "    def __init__(self, searchpath=\"D:\\\\Multi Channel DataManager\\\\\", startfresh=False,\n",
    "                 suffix='.msrd', gcs_credentials_path=\"D:\\\\code\\\\user=ND\\\\divine-builder-142611-9884de65797a.json\",\n",
    "                 gcs_project_id='divine-builder-142611', bucketname='meadata',\n",
    "                 logpath=os.path.join('d:\\\\', 'code', 'user=ND'),\n",
    "                 mcspath=os.path.join('D:\\\\', 'code', 'user=ND', 'McsDataCommandLineConverter',\n",
    "                                      \"McsDataCommandLineConverter.exe\"),\n",
    "                 gcs=0):\n",
    "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = gcs_credentials_path\n",
    "        self.gcs_client = storage.Client(project=gcs_project_id)\n",
    "        self.bucket = self.gcs_client.get_bucket(bucketname)\n",
    "\n",
    "        self.logpath = logpath\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        # create a file handler\n",
    "\n",
    "        self.handler = logging.FileHandler(os.path.join(self.logpath, 'SpreadSheet_RunLog.log'), mode='a')\n",
    "        self.handler.setLevel(logging.INFO)\n",
    "\n",
    "        # create a logging format\n",
    "        self.formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        self.handler.setFormatter(self.formatter)\n",
    "\n",
    "        # add the handlers to the logger\n",
    "        self.logger.addHandler(self.handler)\n",
    "        self.files = []\n",
    "        colnames = ['fullpathmsrd', 'exclude', 'folder', 'coating', 'cleaning', 'MEAfiles', 'recFormat', 'recNames',\n",
    "                    'comments']\n",
    "        self.field = pd.DataFrame(columns=colnames)\n",
    "        self.searchpath = searchpath\n",
    "        self.startfresh = startfresh\n",
    "        self.suffix = suffix\n",
    "        self.numfiles = 0\n",
    "        self.directory = []\n",
    "        self.mcspath = mcspath\n",
    "        self.gcs = gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cud = convertuploaddocument(startfresh=True, gcs = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-28T14-24-14Retina_Trial4.h5 already Exists - skipping upload!\n",
      "2019-02-28T15-18-31Retina_Trial4.h5 already Exists - skipping upload!\n",
      "2019-02-28T15-18-46Retina_Trial4.h5 already Exists - skipping upload!\n",
      "2019-02-28T15-21-04Retina_Trial4.h5 already Exists - skipping upload!\n",
      "2019-02-28T15-32-24Retina_Trial4.h5 already Exists - skipping upload!\n",
      "2019-02-17T14-08-09RetinaSlice_Trial3.h5 already Exists - skipping upload!\n",
      "2019-02-17T14-14-49RetinaSlice_Trial3.h5 already Exists - skipping upload!\n"
     ]
    }
   ],
   "source": [
    "cud.scanforexperiments()\n",
    "cud.converttoh5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
